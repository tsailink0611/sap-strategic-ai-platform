# lambda_function.py
# Stable, no external deps. Reads salesData (array) or csv (string). Bedrock converse. CORS/OPTIONS ready.

import json, os, base64, logging, boto3, urllib.request, urllib.parse
from collections import Counter, defaultdict
from typing import Any, Dict, List, Optional, Tuple

# ====== ENV ======
MODEL_ID       = os.environ.get("BEDROCK_MODEL_ID", "us.deepseek.r1-v1:0")
REGION         = os.environ.get("AWS_REGION", os.environ.get("AWS_DEFAULT_REGION", "us-east-1"))
DEFAULT_FORMAT = (os.environ.get("DEFAULT_FORMAT", "json") or "json").lower()  # 'json'|'markdown'|'text'
MAX_TOKENS     = int(os.environ.get("MAX_TOKENS", "8000"))  # æˆ¦ç•¥ãƒ¬ãƒ™ãƒ«åˆ†æç”¨ã«å¤§å¹…å¢—åŠ 
TEMPERATURE    = float(os.environ.get("TEMPERATURE", "0.15"))
LINE_NOTIFY_TOKEN = os.environ.get("LINE_NOTIFY_TOKEN", "")

# ====== LOG ======
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# ====== CORS/Response ======
def response_json(status: int, body: Dict[str, Any]) -> Dict[str, Any]:
    # Lambda Function URLã®CORSè¨­å®šã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã€Lambdaã§ã¯ãƒ˜ãƒƒãƒ€ãƒ¼è¨­å®šã—ãªã„
    return {
        "statusCode": status,
        "headers": {
            "Content-Type": "application/json; charset=utf-8"
        },
        "body": json.dumps(body, ensure_ascii=False)
    }

# ====== Debug early echo (enable with LAMBDA_DEBUG_ECHO=1 or ?echo=1) ======
def _early_echo(event: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    try:
        qs = (event.get("rawQueryString") or "").lower()
        env_on = os.environ.get("LAMBDA_DEBUG_ECHO") in ("1", "true", "TRUE")
        if not (env_on or ("echo=1" in qs)):
            return None
        body_raw = event.get("body")
        if event.get("isBase64Encoded") and isinstance(body_raw, str):
            try:
                body_raw = base64.b64decode(body_raw).decode("utf-8-sig")
            except Exception:
                body_raw = "<base64 decode error>"
        elif isinstance(body_raw, (bytes, bytearray)):
            try:
                body_raw = body_raw.decode("utf-8-sig")
            except Exception:
                body_raw = body_raw.decode("utf-8", errors="ignore")
        sample = body_raw[:1000] if isinstance(body_raw, str) else str(type(body_raw))
        return response_json(200, {
            "message": "DEBUG",
            "format": "json",
            "engine": "bedrock",
            "model": MODEL_ID,
            "response": {
                "echo": "early",
                "received_type": type(body_raw).__name__ if body_raw is not None else "None",
                "raw_sample": sample
            }
        })
    except Exception:
        return None

# ====== Helpers ======
def _to_number(x: Any) -> float:
    try:
        s = str(x).replace(",", "").replace("Â¥", "").replace("å††", "").strip()
        return float(s)
    except Exception:
        return 0.0

def _detect_columns(rows: List[Dict[str, Any]]) -> Dict[str, str]:
    colmap: Dict[str, str] = {}
    if not rows:
        return colmap
    for c in rows[0].keys():
        name = str(c)
        lc = name.lower()
        if ("æ—¥" in name) or ("date" in lc):
            colmap.setdefault("date", name)
        if ("å£²" in name) or ("é‡‘é¡" in name) or ("amount" in lc) or ("sales" in lc) or ("total" in lc):
            colmap.setdefault("sales", name)
        if ("å•†" in name) or ("å“" in name) or ("product" in lc) or ("item" in lc) or ("name" in lc):
            colmap.setdefault("product", name)
    return colmap

def _compute_stats(rows: List[Dict[str, Any]]) -> Dict[str, Any]:
    total = len(rows)
    if total == 0:
        return {"total_rows": 0, "total_sales": 0.0, "avg_row_sales": 0.0, "top_products": [], "timeseries": []}

    colmap = _detect_columns(rows)
    dcol, scol, pcol = colmap.get("date"), colmap.get("sales"), colmap.get("product")

    ts = defaultdict(float)
    by_product: Counter = Counter()
    total_sales = 0.0

    for r in rows:
        v = _to_number(r.get(scol, 0)) if scol else 0.0
        total_sales += v
        if pcol:
            by_product[str(r.get(pcol, "")).strip()] += v
        if dcol:
            dt = str(r.get(dcol, "")).strip().replace("/", "-")
            day = dt[:10] if len(dt) >= 10 else dt
            if day:
                ts[day] += v

    top_products = [{"name": k, "sales": float(v)} for k, v in by_product.most_common(5)]
    trend = [{"date": d, "sales": float(v)} for d, v in sorted(ts.items())]
    avg = float(total_sales / total) if total else 0.0

    return {
        "total_rows": total,
        "total_sales": float(total_sales),
        "avg_row_sales": avg,
        "top_products": top_products,
        "timeseries": trend
    }

def _build_prompt_json(stats: Dict[str, Any], sample: List[Dict[str, Any]], data_type: str = "sales_data") -> str:
    schema_hint = {
        "type": "object",
        "properties": {
            "overview": {"type": "string"},
            "findings": {"type": "array", "items": {"type": "string"}},
            "kpis": {
                "type": "object",
                "properties": {
                    "total_sales": {"type": "number"},
                    "top_products": {
                        "type": "array",
                        "items": {"type": "object", "properties": {"name": {"type": "string"}, "sales": {"type": "number"}}}
                    }
                }
            },
            "trend": {"type": "array", "items": {"type": "object", "properties": {"date": {"type": "string"}, "sales": {"type": "number"}}}},
            "action_plan": {"type": "array", "items": {"type": "string"}}
        },
        "required": ["overview", "findings", "kpis", "action_plan"]
    }

    # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ¥ã®å®Ÿè·µçš„åˆ†ææŒ‡ç¤º
    analysis_instructions = _get_practical_analysis_instructions(data_type)
    data_type_name = _get_data_type_name(data_type)

    return f"""ã€å®Ÿè·µçš„ãƒ“ã‚¸ãƒã‚¹åˆ†æå®Ÿè¡ŒæŒ‡ä»¤ - å³å®Ÿè¡Œå¯èƒ½ãªæ”¹å–„ææ¡ˆã€‘

ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ: æ—¥æœ¬ã®ä¸­å°ä¼æ¥­çµŒå–¶é™£
åˆ†æå¯¾è±¡: {data_type_name}
åˆ†æã‚¹ã‚¿ã‚¤ãƒ«: æ˜æ—¥ã‹ã‚‰å®Ÿè¡Œã§ãã‚‹å…·ä½“çš„ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³é‡è¦–

ã€å¿…é ˆã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆã€‘
ä»¥ä¸‹ã®å®Ÿè·µçš„åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š

{analysis_instructions}

ã€ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆå½¢å¼ - å¿…ãšå®ˆã£ã¦ãã ã•ã„ã€‘

1. **æ¦‚è¦** (overview)
   - ãƒ‡ãƒ¼ã‚¿ã®é‡è¦ãªç™ºè¦‹ã‚’3è¡Œä»¥å†…ã§è¦ç´„
   - æœ€ã‚‚é‡è¦ãªæ”¹å–„æ©Ÿä¼šã‚’1ã¤æ˜ç¢ºã«ç‰¹å®š
   - å…·ä½“çš„ãªé‡‘é¡åŠ¹æœã‚’å¿…ãšè¨˜è¼‰ï¼ˆä¾‹ï¼š"æœˆâ—‹â—‹ä¸‡å††ã®å£²ä¸Šå‘ä¸ŠãŒæœŸå¾…"ï¼‰

2. **é‡è¦ãªç™ºè¦‹** (findings)
   - ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰èª­ã¿å–ã‚Œã‚‹å…·ä½“çš„äº‹å®Ÿã‚’5å€‹ä»¥å†…ã§åˆ—æŒ™
   - å„ç™ºè¦‹ã«å¿…ãšæ•°å€¤ã‚’å«ã‚ã‚‹
   - æ”¹å–„ã™ã¹ãå•é¡Œç‚¹ã‚’æ˜ç¢ºã«æŒ‡æ‘˜

3. **ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³** (action_plan)  â† æœ€é‡è¦ï¼
   - æ˜æ—¥ã‹ã‚‰å®Ÿè¡Œã§ãã‚‹å…·ä½“çš„ãªè¡Œå‹•ã‚’5-7å€‹æç¤º
   - å„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã«å®Ÿè¡ŒæœŸé™ãƒ»æ‹…å½“è€…ãƒ»æœŸå¾…åŠ¹æœã‚’æ˜è¨˜
   - ä¾‹ï¼š"å–¶æ¥­éƒ¨é•·ã¯æ¥é€±ã¾ã§ã«â—‹â—‹å•†å“ã®å˜ä¾¡ã‚’500å††å€¤ä¸Šã’æ¤œè¨ï¼ˆæœˆå£²ä¸Š20ä¸‡å††å‘ä¸Šè¦‹è¾¼ã¿ï¼‰"
   - å®Ÿè¡Œã‚³ã‚¹ãƒˆã¨åŠ¹æœã‚’å¿…ãšæ•°å€¤ã§ç¤ºã™

ã€çµ¶å¯¾ã«é¿ã‘ã‚‹ã“ã¨ã€‘
Ã— æŠ½è±¡çš„ãªææ¡ˆï¼ˆ"æˆ¦ç•¥ã‚’è¦‹ç›´ã™"ãªã©ï¼‰
Ã— å®Ÿè¡ŒæœŸé™ã®ãªã„ææ¡ˆ
Ã— é‡‘é¡åŠ¹æœã®è¨˜è¼‰ãŒãªã„ææ¡ˆ
Ã— å¤§ä¼æ¥­å‘ã‘ã®é«˜é¡æŠ•è³‡ãŒå¿…è¦ãªææ¡ˆ

ã€å¿…é ˆè¦ä»¶ã€‘
âœ“ å…¨ææ¡ˆãŒä¸­å°ä¼æ¥­ã§å³å®Ÿè¡Œå¯èƒ½
âœ“ å„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã«å…·ä½“çš„ãªæ•°å€¤ç›®æ¨™
âœ“ ä½ã‚³ã‚¹ãƒˆãƒ»é«˜åŠ¹æœã®æ”¹å–„æ¡ˆå„ªå…ˆ
âœ“ è²¬ä»»è€…ãƒ»æœŸé™ãƒ»KPIã‚’æ˜ç¢ºåŒ–
âœ“ ROIï¼ˆæŠ•è³‡å¯¾åŠ¹æœï¼‰ã‚’é‡‘é¡ã§æ˜ç¤º

JSONå½¢å¼ã§å‡ºåŠ›: {json.dumps(schema_hint, ensure_ascii=False)}

ã€åˆ†æãƒ‡ãƒ¼ã‚¿ã€‘
çµ±è¨ˆã‚µãƒãƒªãƒ¼: {json.dumps(stats, ensure_ascii=False)}
ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿: {json.dumps(sample, ensure_ascii=False)}

â€»ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯çµŒå–¶é™£ãŒèª­ã‚“ã ç¿Œæ—¥ã‹ã‚‰å®Ÿè¡Œã«ç§»ã›ã‚‹å®Ÿç”¨æ€§ã‚’æœ€å„ªå…ˆã—ã¦ãã ã•ã„ã€‚"""

def _build_prompt_markdown(stats: Dict[str, Any], sample: List[Dict[str, Any]], data_type: str = "sales_data") -> str:
    return f"""ã‚ãªãŸã¯ä¼šç¤¾ã®å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã™ã‚‹ãƒ“ã‚¸ãƒã‚¹ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã§ã™ã€‚ä»¥ä¸‹ã®å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã‚’è¦‹ã¦ã€ç¤¾é•·ã‚„éƒ¨é•·ãŒèª­ã‚€ãƒ¬ãƒãƒ¼ãƒˆã‚’ã€å®Œå…¨ã«æ—¥æœ¬èªã¨æ•°å­—ã ã‘ã§ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã€é‡è¦ã€‘
- Markdownã‚„è¨˜å·ã¯ä¸€åˆ‡ä½¿ã‚ãšã€æ™®é€šã®æ—¥æœ¬èªæ–‡ç« ã§æ›¸ã„ã¦ãã ã•ã„
- ã€Œ##ã€ã€Œ**ã€ã€Œ|ã€ã€Œ-ã€ãªã©ã®è¨˜å·ã¯çµ¶å¯¾ã«ä½¿ã‚ãªã„ã§ãã ã•ã„
- è‹±èªã‚„å°‚é–€ç”¨èªã¯ä¸€åˆ‡ä½¿ã‚ãªã„ã§ãã ã•ã„
- ã¾ã‚‹ã§éƒ¨ä¸‹ãŒä¸Šå¸ã«å£é ­ã§å ±å‘Šã™ã‚‹ã‚ˆã†ãªã€è‡ªç„¶ãªæ–‡ç« ã§æ›¸ã„ã¦ãã ã•ã„
- æ•°å­—ã¯ã€Œâ—‹â—‹ä¸‡å††ã€ã€Œâ—‹â—‹%å¢—åŠ ã€ãªã©ã€æ—¥æœ¬äººãŒè©±ã™ã¨ãã®è¡¨ç¾ã§æ›¸ã„ã¦ãã ã•ã„

# çµ±è¨ˆè¦ç´„
{json.dumps(stats, ensure_ascii=False)}

# ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€å¤§50ï¼‰
{json.dumps(sample, ensure_ascii=False)}
"""

def _build_prompt_text(stats: Dict[str, Any], sample: List[Dict[str, Any]], data_type: str = "sales_data") -> str:
    return f"""ã‚ãªãŸã¯ä¼šç¤¾ã®å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã™ã‚‹ãƒ“ã‚¸ãƒã‚¹ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã§ã™ã€‚ä»¥ä¸‹ã®å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã‚’è¦‹ã¦ã€ä¸Šå¸ã«å£é ­ã§å ±å‘Šã™ã‚‹ã‚ˆã†ã«ã€å®Œå…¨ã«æ—¥æœ¬èªã ã‘ã§3è¡Œä»¥å†…ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚

ã€çµ¶å¯¾å®ˆã‚‹ã“ã¨ã€‘
- è¨˜å·ã€è‹±èªã€ã‚«ã‚¿ã‚«ãƒŠå°‚é–€ç”¨èªã¯ä¸€åˆ‡ä½¿ã‚ãªã„ã§ãã ã•ã„
- æ•°å­—ã¯ã€Œâ—‹â—‹ä¸‡å††ã€ã€Œâ—‹â—‹%å¢—åŠ ã€ãªã©ã€æ™®é€šã«è©±ã™ã¨ãã®è¡¨ç¾ã§æ›¸ã„ã¦ãã ã•ã„
- ã¾ã‚‹ã§æœç¤¼ã§å ±å‘Šã™ã‚‹ã‚ˆã†ãªã€è‡ªç„¶ãªè©±ã—è¨€è‘‰ã§æ›¸ã„ã¦ãã ã•ã„
- ã€Œã§ã™ãƒ»ã¾ã™ã€èª¿ã§ã€ä¸å¯§ã«æ›¸ã„ã¦ãã ã•ã„

[çµ±è¨ˆè¦ç´„]
{json.dumps(stats, ensure_ascii=False)}

[ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€å¤§50ï¼‰]
{json.dumps(sample, ensure_ascii=False)}
"""

def _parse_csv_simple(csv_text: str) -> List[Dict[str, Any]]:
    lines = [l for l in csv_text.splitlines() if l.strip() != ""]
    if not lines: return []
    headers = [h.strip() for h in lines[0].split(",")]
    rows: List[Dict[str, Any]] = []
    for line in lines[1:]:
        cells = [c.strip() for c in line.split(",")]
        row = {}
        for i, h in enumerate(headers):
            row[h] = cells[i] if i < len(cells) else ""
        rows.append(row)
    return rows

def _identify_data_type(columns: List[str], sample_data: List[Dict[str, Any]]) -> str:
    """ãƒ‡ãƒ¼ã‚¿ã®åˆ—åã¨ã‚µãƒ³ãƒ—ãƒ«ã‹ã‚‰è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã®ç¨®é¡ã‚’è‡ªå‹•åˆ¤åˆ¥ï¼ˆ7ã¤ã®åˆ†æã‚¿ã‚¤ãƒ—ã«ç‰¹åŒ–ï¼‰"""
    if not columns:
        return "financial_data"
    
    # åˆ—åã‚’å°æ–‡å­—ã«å¤‰æ›ã—ã¦åˆ¤åˆ¥ã—ã‚„ã™ãã™ã‚‹
    col_lower = [col.lower() for col in columns]
    col_str = " ".join(col_lower) + " " + " ".join(columns)
    
    # ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹ã®åˆ¤å®šã‚·ã‚¹ãƒ†ãƒ 
    scores = {
        "hr_data": 0,
        "marketing_data": 0,
        "sales_data": 0,
        "financial_data": 0,
        "inventory_data": 0,
        "customer_data": 0
    }
    
    # äººäº‹ãƒ‡ãƒ¼ã‚¿ã®å¼·ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆé«˜ã‚¹ã‚³ã‚¢ï¼‰
    hr_strong_keywords = ["ç¤¾å“¡id", "employee", "æ°å", "éƒ¨ç½²", "çµ¦ä¸", "salary", "è³ä¸", "å¹´å", "è©•ä¾¡", "performance", "æ®‹æ¥­", "overtime", "æœ‰çµ¦", "é›¢è·", "æ˜‡é€²", "ã‚¹ã‚­ãƒ«", "ãƒãƒ¼ãƒ è²¢çŒ®", "äººäº‹"]
    for keyword in hr_strong_keywords:
        if keyword in col_str:
            scores["hr_data"] += 3
    
    # äººäº‹ãƒ‡ãƒ¼ã‚¿ã®ä¸­ç¨‹åº¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    hr_medium_keywords = ["å‹¤æ€ ", "attendance", "ç ”ä¿®", "training", "ç›®æ¨™é”æˆ", "è·ä½", "å…¥ç¤¾", "å¹´é½¢"]
    for keyword in hr_medium_keywords:
        if keyword in col_str:
            scores["hr_data"] += 2
    
    # ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®å¼·ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    marketing_strong_keywords = ["ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³", "campaign", "roi", "ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³", "impression", "ã‚¯ãƒªãƒƒã‚¯", "click", "cvæ•°", "conversion", "é¡§å®¢ç²å¾—", "cac", "roas", "åºƒå‘Š", "åª’ä½“", "ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ"]
    for keyword in marketing_strong_keywords:
        if keyword in col_str:
            scores["marketing_data"] += 3
    
    # ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®ä¸­ç¨‹åº¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    marketing_medium_keywords = ["äºˆç®—", "budget", "æ”¯å‡º", "cost", "facebook", "google", "youtube", "instagram", "tiktok", "twitter"]
    for keyword in marketing_medium_keywords:
        if keyword in col_str:
            scores["marketing_data"] += 1
    
    # å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã®å¼·ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    sales_strong_keywords = ["å£²ä¸Š", "sales", "revenue", "å•†å“", "product", "é¡§å®¢", "customer", "é‡‘é¡", "amount", "å˜ä¾¡", "price", "æ•°é‡", "quantity"]
    for keyword in sales_strong_keywords:
        if keyword in col_str:
            scores["sales_data"] += 3
    
    # å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã®ä¸­ç¨‹åº¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    sales_medium_keywords = ["æ—¥ä»˜", "date", "åº—èˆ—", "store", "åœ°åŸŸ", "region", "ã‚«ãƒ†ã‚´ãƒª", "category"]
    for keyword in sales_medium_keywords:
        if keyword in col_str:
            scores["sales_data"] += 1
    
    # çµ±åˆæˆ¦ç•¥ãƒ‡ãƒ¼ã‚¿ï¼ˆè²¡å‹™ãƒ‡ãƒ¼ã‚¿ï¼‰ã®å¼·ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    financial_strong_keywords = ["å£²ä¸Šé«˜", "revenue", "åˆ©ç›Š", "profit", "è³‡ç”£", "asset", "è² å‚µ", "liability", "ã‚­ãƒ£ãƒƒã‚·ãƒ¥", "cash", "æç›Š", "pl", "è²¸å€Ÿ", "bs"]
    for keyword in financial_strong_keywords:
        if keyword in col_str:
            scores["financial_data"] += 3
    
    # åœ¨åº«åˆ†æãƒ‡ãƒ¼ã‚¿ã®å¼·ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    inventory_strong_keywords = ["åœ¨åº«", "inventory", "stock", "åœ¨åº«æ•°", "ä¿æœ‰æ•°", "å€‰åº«", "warehouse", "å›è»¢ç‡", "turnover", "æ»ç•™", "å…¥åº«", "å‡ºåº«", "èª¿é”", "procurement"]
    for keyword in inventory_strong_keywords:
        if keyword in col_str:
            scores["inventory_data"] += 3
    
    # åœ¨åº«åˆ†æãƒ‡ãƒ¼ã‚¿ã®ä¸­ç¨‹åº¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    inventory_medium_keywords = ["å•†å“ã‚³ãƒ¼ãƒ‰", "sku", "ãƒ­ãƒƒãƒˆ", "lot", "å“ç•ª", "å‹ç•ª", "ä»•å…¥", "supplier", "ç™ºæ³¨", "order", "ç´æœŸ", "delivery"]
    for keyword in inventory_medium_keywords:
        if keyword in col_str:
            scores["inventory_data"] += 1
    
    # é¡§å®¢åˆ†æãƒ‡ãƒ¼ã‚¿ã®å¼·ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰  
    customer_strong_keywords = ["é¡§å®¢", "customer", "ä¼šå“¡", "member", "ãƒ¦ãƒ¼ã‚¶ãƒ¼", "user", "ltv", "lifetime", "churn", "é›¢è„±", "ç¶™ç¶š", "retention", "æº€è¶³åº¦", "satisfaction"]
    for keyword in customer_strong_keywords:
        if keyword in col_str:
            scores["customer_data"] += 3
    
    # é¡§å®¢åˆ†æãƒ‡ãƒ¼ã‚¿ã®ä¸­ç¨‹åº¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    customer_medium_keywords = ["ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ", "segment", "å¹´é½¢", "age", "æ€§åˆ¥", "gender", "åœ°åŸŸ", "region", "è³¼å…¥å±¥æ­´", "purchase", "ã‚¢ã‚¯ã‚»ã‚¹", "access", "ã‚¯ãƒªãƒƒã‚¯", "click"]
    for keyword in customer_medium_keywords:
        if keyword in col_str:
            scores["customer_data"] += 1
    
    # ãƒ‡ãƒ¼ã‚¿ã®å†…å®¹ã‹ã‚‰ã‚‚åˆ¤å®šï¼ˆã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
    if sample_data and len(sample_data) > 0:
        sample = sample_data[0]
        
        # äººäº‹ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´çš„ãªå€¤ãƒ‘ã‚¿ãƒ¼ãƒ³
        for key, value in sample.items():
            str_value = str(value).lower()
            
            # äººäº‹ç³»ã®å€¤ãƒ‘ã‚¿ãƒ¼ãƒ³
            if any(dept in str_value for dept in ["å–¶æ¥­éƒ¨", "itéƒ¨", "äººäº‹éƒ¨", "è²¡å‹™éƒ¨", "ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°éƒ¨"]):
                scores["hr_data"] += 5
            if any(pos in str_value for pos in ["ä¸»ä»»", "ä¿‚é•·", "ä¸€èˆ¬", "éƒ¨é•·", "èª²é•·"]):
                scores["hr_data"] += 3
            if any(risk in str_value for risk in ["ä½", "ä¸­", "é«˜"]) and ("ãƒªã‚¹ã‚¯" in key or "risk" in key.lower()):
                scores["hr_data"] += 4
                
            # ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ç³»ã®å€¤ãƒ‘ã‚¿ãƒ¼ãƒ³
            if any(media in str_value for media in ["googleåºƒå‘Š", "facebookåºƒå‘Š", "youtubeåºƒå‘Š", "instagramåºƒå‘Š", "lineåºƒå‘Š", "tiktokåºƒå‘Š"]):
                scores["marketing_data"] += 5
            if "%" in str_value and any(metric in key.lower() for metric in ["roi", "é”æˆç‡", "æº€è¶³åº¦"]):
                scores["marketing_data"] += 2
                
            # å£²ä¸Šç³»ã®å€¤ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ•°å€¤ãŒå¤§ããã€å•†å“åãŒã‚ã‚‹å ´åˆï¼‰
            if "å•†å“" in key or "product" in key.lower():
                scores["sales_data"] += 3
            if key.lower() in ["åº—èˆ—", "store"] and str_value:
                scores["sales_data"] += 4
                
            # åœ¨åº«ç³»ã®å€¤ãƒ‘ã‚¿ãƒ¼ãƒ³
            if any(unit in str_value for unit in ["å€‹", "æœ¬", "kg", "ç®±", "ã‚»ãƒƒãƒˆ", "å°"]):
                scores["inventory_data"] += 2
            if "warehouse" in key.lower() or "å€‰åº«" in key:
                scores["inventory_data"] += 3
            if any(status in str_value for status in ["å…¥è·å¾…ã¡", "å‡ºè·æ¸ˆã¿", "åœ¨åº«åˆ‡ã‚Œ", "èª¿é”ä¸­"]):
                scores["inventory_data"] += 4
                
            # é¡§å®¢ç³»ã®å€¤ãƒ‘ã‚¿ãƒ¼ãƒ³  
            if any(age in str_value for age in ["20ä»£", "30ä»£", "40ä»£", "50ä»£", "60ä»£"]) or str_value.isdigit() and 18 <= int(str_value) <= 80:
                scores["customer_data"] += 3
            if any(gender in str_value for gender in ["ç”·æ€§", "å¥³æ€§", "male", "female", "ç”·", "å¥³"]):
                scores["customer_data"] += 3
            if "@" in str_value:  # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹
                scores["customer_data"] += 4
    
    # æœ€é«˜ã‚¹ã‚³ã‚¢ã®ã‚¿ã‚¤ãƒ—ã‚’è¿”ã™
    if max(scores.values()) > 0:
        return max(scores, key=scores.get)
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    return "financial_data"

def _get_data_type_name(data_type: str) -> str:
    """ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã®æ—¥æœ¬èªåã‚’è¿”ã™"""
    type_names = {
        "pl_statement": "æç›Šè¨ˆç®—æ›¸ï¼ˆPLè¡¨ï¼‰",
        "balance_sheet": "è²¸å€Ÿå¯¾ç…§è¡¨ï¼ˆBSï¼‰",
        "cashflow_statement": "ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼è¨ˆç®—æ›¸",
        "sales_data": "å£²ä¸Šãƒ‡ãƒ¼ã‚¿",
        "inventory_data": "åœ¨åº«ãƒ‡ãƒ¼ã‚¿",
        "customer_data": "é¡§å®¢ãƒ‡ãƒ¼ã‚¿",
        "hr_data": "äººäº‹ãƒ‡ãƒ¼ã‚¿",
        "marketing_data": "ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿",
        "financial_data": "è²¡å‹™ãƒ‡ãƒ¼ã‚¿",
        "document_data": "æ›¸é¡ç”»åƒãƒ‡ãƒ¼ã‚¿",
        "unknown": "ä¸æ˜ãªãƒ‡ãƒ¼ã‚¿"
    }
    return type_names.get(data_type, "è²¡å‹™ãƒ‡ãƒ¼ã‚¿")

def validate_analysis_compatibility(detected_data_type: str, requested_analysis_type: str) -> Tuple[bool, str]:
    """ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã¨åˆ†æã‚¿ã‚¤ãƒ—ã®é©åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆä½¿ã„ã‚„ã™ã•é‡è¦–ï¼‰"""
    # é©åˆæ€§ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ - ã‚ˆã‚ŠæŸ”è»Ÿã«
    compatibility_matrix = {
        'sales': {
            'primary': ['sales_data'],  # ä¸»è¦å¯¾å¿œ
            'secondary': ['financial_data'],  # å‰¯æ¬¡å¯¾å¿œï¼ˆè­¦å‘Šãªã—ã§é€šã™ï¼‰
            'name': 'å£²ä¸Šåˆ†æ',
            'description': 'å£²ä¸Šãƒ»å•†å“ãƒ»é¡§å®¢ãƒ‡ãƒ¼ã‚¿ã®åˆ†æ'
        },
        'hr': {
            'primary': ['hr_data'],
            'secondary': [],  # äººäº‹ã¯å³å¯†ã«
            'name': 'äººäº‹åˆ†æ', 
            'description': 'å¾“æ¥­å“¡ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»çµ¦ä¸ãƒ»è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã®åˆ†æ'
        },
        'marketing': {
            'primary': ['marketing_data'],
            'secondary': ['financial_data'],  # äºˆç®—ãƒ‡ãƒ¼ã‚¿ãªã©ã‚‚å¯
            'name': 'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°åˆ†æ',
            'description': 'ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ãƒ»ROIãƒ»é¡§å®¢ç²å¾—ãƒ‡ãƒ¼ã‚¿ã®åˆ†æ'
        },
        'strategic': {
            'primary': ['financial_data', 'sales_data'],
            'secondary': ['hr_data', 'marketing_data'],  # çµ±åˆæˆ¦ç•¥ã¯ä½•ã§ã‚‚å¯
            'name': 'çµ±åˆæˆ¦ç•¥åˆ†æ',
            'description': 'ç·åˆçš„ãªãƒ“ã‚¸ãƒã‚¹ãƒ‡ãƒ¼ã‚¿ã®æˆ¦ç•¥åˆ†æ'
        }
    }
    
    # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¿ã‚¤ãƒ—ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯é€šã™
    if requested_analysis_type not in compatibility_matrix:
        return True, ""
    
    config = compatibility_matrix[requested_analysis_type]
    
    # ä¸»è¦ã‚¿ã‚¤ãƒ—ã¾ãŸã¯å‰¯æ¬¡ã‚¿ã‚¤ãƒ—ã«é©åˆã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    all_allowed = config['primary'] + config['secondary']
    
    if detected_data_type in all_allowed:
        return True, ""  # é©åˆã—ã¦ã„ã‚‹
    
    # ä¸é©åˆã®å ´åˆã®ã¿ã‚¨ãƒ©ãƒ¼
    if detected_data_type not in all_allowed:
        # æœ€é©ãªãƒœã‚¿ãƒ³ã‚’ææ¡ˆ
        best_match = None
        for btn_type, btn_config in compatibility_matrix.items():
            if detected_data_type in (btn_config['primary'] + btn_config['secondary']):
                best_match = btn_config['name']
                break
        
        error_msg = f"""âš ï¸ ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã®ä¸ä¸€è‡´ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ

ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿: {_get_data_type_name(detected_data_type)}
é¸æŠã•ã‚ŒãŸåˆ†æ: {config['name']}

ã“ã®ãƒ‡ãƒ¼ã‚¿ã¯{config['name']}ã«ã¯æœ€é©åŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"""
        
        if best_match:
            error_msg += f"\n\nğŸ’¡ ã“ã®ãƒ‡ãƒ¼ã‚¿ã«ã¯ã€Œ{best_match}ã€ãŒãŠã™ã™ã‚ã§ã™ã€‚\n\nãŸã ã—ã€ãã®ã¾ã¾åˆ†æã‚’ç¶šè¡Œã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ã€‚"
            # è­¦å‘Šã ã‘ã§ç¶šè¡Œã‚’è¨±å¯
            return True, ""
        else:
            error_msg += f"\n\nã€Œçµ±åˆæˆ¦ç•¥åˆ†æã€ãƒœã‚¿ãƒ³ã‚’ãŠè©¦ã—ãã ã•ã„ã€‚"
            return True, ""
    
    return True, ""

def _get_practical_analysis_instructions(data_type: str) -> str:
    """ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ¥ã®å®Ÿè·µçš„åˆ†ææŒ‡ç¤ºã‚’è¿”ã™"""
    instructions = {
        "pl_statement": """
**å³åŠ¹æ€§ã®ã‚ã‚‹è²¡å‹™æ”¹å–„åˆ†æ**
- ç²—åˆ©ç‡ã®ä½ã„å•†å“ãƒ»ã‚µãƒ¼ãƒ“ã‚¹ã‚’ç‰¹å®šã—ã€ä¾¡æ ¼è¦‹ç›´ã—ã¾ãŸã¯åŸä¾¡å‰Šæ¸›ã®å…·ä½“æ¡ˆ
- è²©ç®¡è²»ã§å‰Šæ¸›å¯èƒ½ãªé …ç›®ãƒˆãƒƒãƒ—3ã¨å‰Šæ¸›é‡‘é¡ã‚’ç®—å‡º
- å–¶æ¥­åˆ©ç›Šç‡ã‚’2%å‘ä¸Šã•ã›ã‚‹ãŸã‚ã®å…·ä½“çš„æ–½ç­–
- æ¥æœˆã‹ã‚‰å®Ÿè¡Œã§ãã‚‹ã‚³ã‚¹ãƒˆå‰Šæ¸›æ¡ˆï¼ˆé‡‘é¡åŠ¹æœä»˜ãï¼‰""",

        "balance_sheet": """
**è³‡é‡‘ç¹°ã‚Šæ”¹å–„ã®å®Ÿè·µçš„ææ¡ˆ**
- å£²æ›é‡‘å›åã‚µã‚¤ãƒˆçŸ­ç¸®ã«ã‚ˆã‚‹è³‡é‡‘ç¹°ã‚Šæ”¹å–„åŠ¹æœã‚’è¨ˆç®—
- åœ¨åº«å‰Šæ¸›ã§æ»å‡ºã§ãã‚‹è³‡é‡‘é¡ã¨å…·ä½“çš„å‰Šæ¸›å¯¾è±¡
- æµå‹•æ¯”ç‡æ”¹å–„ã®ãŸã‚ã®å³åŠ¹æ€§ã‚ã‚‹æ–½ç­–
- å€Ÿå…¥é‡‘åˆ©è² æ‹…è»½æ¸›ã®ãŸã‚ã®é‡‘èæ©Ÿé–¢äº¤æ¸‰ãƒã‚¤ãƒ³ãƒˆ""",

        "cashflow_statement": """
**ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼æ”¹å–„ã®å…·ä½“çš„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³**
- å›åã‚µã‚¤ãƒˆãƒ»æ”¯æ‰•ã‚µã‚¤ãƒˆè¦‹ç›´ã—ã«ã‚ˆã‚‹è³‡é‡‘ç¹°ã‚Šæ”¹å–„é¡
- ä¸è¦ãªè¨­å‚™æŠ•è³‡ã®è¦‹ç›´ã—å¯¾è±¡ã¨ç¯€ç´„åŠ¹æœ
- å–¶æ¥­CFã‚’æœˆâ—‹â—‹ä¸‡å††æ”¹å–„ã™ã‚‹ãŸã‚ã®å…·ä½“çš„æ‰‹é †
- è³‡é‡‘ã‚·ãƒ§ãƒ¼ãƒˆå›é¿ã®ãŸã‚ã®ç·Šæ€¥å¯¾å¿œç­–""",
        
        "sales_data": """
**å³åŠ¹æ€§å£²ä¸Šæ”¹å–„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³**

**ä»Šæœˆå®Ÿè¡Œå¯èƒ½ãªå£²ä¸Šå‘ä¸Šç­–**
- å£²ä¸ŠTOPå•†å“ã®å˜ä¾¡ã‚’æ®µéšçš„ã«5-10%å€¤ä¸Šã’ã—ãŸå ´åˆã®å¢—ååŠ¹æœã‚’è¨ˆç®—
- ä½åç›Šå•†å“ã®è²©å£²ä¸­æ­¢ãƒ»ä¾¡æ ¼æ”¹å®šã«ã‚ˆã‚‹åˆ©ç›Šæ”¹å–„é¡
- å„ªè‰¯é¡§å®¢ã¸ã®è¿½åŠ å•†å“ææ¡ˆã§ç²å¾—ã§ãã‚‹å£²ä¸Šé¡ï¼ˆå…·ä½“çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒæ–¹æ³•ä»˜ãï¼‰
- å–¶æ¥­åŠ¹ç‡ã®æ‚ªã„å•†å“ãƒ»é¡§å®¢ã®è¦‹ç›´ã—ã«ã‚ˆã‚‹æ™‚é–“å½“ãŸã‚Šå£²ä¸Šå‘ä¸Š

**3ãƒ¶æœˆä»¥å†…ã®å–¶æ¥­æ”¹å–„è¨ˆç”»**
- æˆç´„ç‡å‘ä¸Šã®ãŸã‚ã®å–¶æ¥­ãƒ—ãƒ­ã‚»ã‚¹æ”¹å–„ï¼ˆå…·ä½“çš„æ‰‹é †ã¨æœŸå¾…åŠ¹æœï¼‰
- ãƒªãƒ”ãƒ¼ãƒˆç‡å‘ä¸Šæ–½ç­–ï¼ˆã‚³ã‚¹ãƒˆãƒ»å®Ÿè¡Œæ–¹æ³•ãƒ»åŠ¹æœæ¸¬å®šæ–¹æ³•ï¼‰
- æ–°è¦é–‹æ‹“ã™ã¹ãé¡§å®¢å±¤ã®ç‰¹å®šã¨å…·ä½“çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒæ‰‹é †
- å–¶æ¥­æ‹…å½“è€…åˆ¥ã®æ”¹å–„ãƒã‚¤ãƒ³ãƒˆã¨ç ”ä¿®å†…å®¹

**æ•°å€¤æ”¹å–„ç›®æ¨™ã®è¨­å®š**
- æœˆæ¬¡å£²ä¸Šç›®æ¨™ã‚’é”æˆã™ã‚‹ãŸã‚ã«å¿…è¦ãªå…·ä½“çš„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•°
- å®¢å˜ä¾¡ãƒ»æˆç´„ç‡ãƒ»ãƒªãƒ”ãƒ¼ãƒˆç‡ã®æ”¹å–„ã«ã‚ˆã‚‹å£²ä¸Šã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆè©¦ç®—
- å–¶æ¥­ã‚³ã‚¹ãƒˆå‰Šæ¸›ã¨å£²ä¸ŠåŠ¹ç‡åŒ–ã®ä¸¡ç«‹æ¡ˆ
- ç«¶åˆå¯¾ç­–ã¨ã—ã¦å³åº§ã«å®Ÿè¡Œã™ã¹ãå·®åˆ¥åŒ–æ–½ç­–""",
        
        "inventory_data": """
- åœ¨åº«ã®ç·é¡ã€å•†å“åˆ¥æ§‹æˆã‚’ç¢ºèªã—ã¦ãã ã•ã„
- åœ¨åº«å›è»¢ç‡ã‚„æ»ç•™åœ¨åº«ãŒã‚ã‚Œã°æŒ‡æ‘˜ã—ã¦ãã ã•ã„
- é©æ­£åœ¨åº«ãƒ¬ãƒ™ãƒ«ã¨éå‰°åœ¨åº«ã®ãƒªã‚¹ã‚¯ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„
- åœ¨åº«ç®¡ç†ã®æ”¹å–„ç‚¹ãŒã‚ã‚Œã°ææ¡ˆã—ã¦ãã ã•ã„""",
        
        "hr_data": """
**å³åŠ¹æ€§äººäº‹æ”¹å–„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³**

**ä»Šæœˆå®Ÿè¡Œå¯èƒ½ãªç”Ÿç”£æ€§å‘ä¸Šç­–**
- æ®‹æ¥­æ™‚é–“å‰Šæ¸›ã«ã‚ˆã‚‹äººä»¶è²»å‰Šæ¸›é¡ã¨å…·ä½“çš„æ™‚çŸ­æ–½ç­–
- ä½ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ¼ç¤¾å“¡ã¸ã®å…·ä½“çš„æ”¹å–„æŒ‡å°ãƒ—ãƒ©ãƒ³ï¼ˆæœŸé™ãƒ»ç›®æ¨™è¨­å®šï¼‰
- é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ¼ç¤¾å“¡ã®é›¢è·é˜²æ­¢ç­–ï¼ˆæ˜‡çµ¦ãƒ»æ˜‡æ ¼ãƒ»ç‰¹åˆ¥æ‰‹å½“ã®å…·ä½“æ¡ˆï¼‰
- éƒ¨ç½²é–“ã®äººå“¡é…ç½®è¦‹ç›´ã—ã«ã‚ˆã‚‹æ¥­å‹™åŠ¹ç‡åŒ–

**3ãƒ¶æœˆä»¥å†…ã®äººäº‹ã‚³ã‚¹ãƒˆæœ€é©åŒ–**
- å¤–éƒ¨å§”è¨—vså†…è£½åŒ–ã®åˆ‡ã‚Šæ›¿ãˆã«ã‚ˆã‚‹ã‚³ã‚¹ãƒˆå‰Šæ¸›åŠ¹æœ
- ç ”ä¿®è²»ç”¨å¯¾åŠ¹æœã®è¦‹ç›´ã—ã¨å„ªå…ˆé †ä½ä»˜ã‘
- è©•ä¾¡åˆ¶åº¦æ”¹å–„ã«ã‚ˆã‚‹ç¤¾å“¡ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³å‘ä¸Šæ–½ç­–
- æ¡ç”¨ã‚³ã‚¹ãƒˆå‰Šæ¸›ã®ãŸã‚ã®ç´¹ä»‹åˆ¶åº¦ãƒ»ãƒªãƒ•ã‚¡ãƒ©ãƒ«å¼·åŒ–

**äººæãƒªã‚¹ã‚¯ç®¡ç†ã®å®Ÿè·µç­–**
- é›¢è·ãƒªã‚¹ã‚¯ã®é«˜ã„ç¤¾å“¡ã¸ã®å…·ä½“çš„æ…°ç•™ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
- æ¥­å‹™å±äººåŒ–è§£æ¶ˆã®ãŸã‚ã®ãƒãƒ‹ãƒ¥ã‚¢ãƒ«åŒ–ãƒ»å¼•ç¶™ãä½“åˆ¶
- ç®¡ç†è·ã®äººäº‹è©•ä¾¡ã‚¹ã‚­ãƒ«å‘ä¸Šã®ãŸã‚ã®å®Ÿè·µç ”ä¿®
- çµ¦ä¸ãƒ»è³ä¸ã®é©æ­£åŒ–ã«ã‚ˆã‚‹äººä»¶è²»é…åˆ†æœ€é©åŒ–""",
        
        "marketing_data": """
**å³åŠ¹æ€§ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°æ”¹å–„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³**

**ä»Šæœˆå®Ÿè¡Œå¯èƒ½ãªåºƒå‘ŠåŠ¹ç‡åŒ–**
- ROASã®ä½ã„åºƒå‘Šåª’ä½“ãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®åœæ­¢ã«ã‚ˆã‚‹ç„¡é§„ã‚³ã‚¹ãƒˆå‰Šæ¸›é¡
- é«˜æˆæœåºƒå‘Šã®äºˆç®—å¢—é¡ã«ã‚ˆã‚‹å£²ä¸Šå‘ä¸Šè¦‹è¾¼ã¿ï¼ˆå…·ä½“çš„é‡‘é¡é…åˆ†ï¼‰
- CPAï¼ˆé¡§å®¢ç²å¾—å˜ä¾¡ï¼‰æ”¹å–„ã®ãŸã‚ã®åºƒå‘Šæ–‡ãƒ»ã‚¿ãƒ¼ã‚²ãƒ†ã‚£ãƒ³ã‚°è¦‹ç›´ã—
- ç„¡æ–™æ–½ç­–ï¼ˆSNSãƒ»å£ã‚³ãƒŸãƒ»ç´¹ä»‹åˆ¶åº¦ï¼‰ã§ä»£æ›¿å¯èƒ½ãªæœ‰æ–™åºƒå‘Šã®ç‰¹å®š

**3ãƒ¶æœˆä»¥å†…ã®é¡§å®¢ç²å¾—æœ€é©åŒ–**
- æ–°è¦é¡§å®¢ç²å¾—ã‚³ã‚¹ãƒˆã¨æ—¢å­˜é¡§å®¢ç¶­æŒã‚³ã‚¹ãƒˆã®æœ€é©é…åˆ†
- ãƒªãƒ”ãƒ¼ãƒˆç‡å‘ä¸Šæ–½ç­–ï¼ˆãƒ¡ãƒ«ãƒã‚¬ãƒ»LINEãƒ»ä¼šå“¡ç‰¹å…¸ï¼‰ã®å…·ä½“çš„å®Ÿè¡Œãƒ—ãƒ©ãƒ³
- é«˜LTVé¡§å®¢ã®ç‰¹å¾´åˆ†æã¨åŒæ§˜é¡§å®¢ã®ç²å¾—ã‚¿ãƒ¼ã‚²ãƒ†ã‚£ãƒ³ã‚°
- ã‚¯ãƒ­ã‚¹ã‚»ãƒ«ãƒ»ã‚¢ãƒƒãƒ—ã‚»ãƒ«ã«ã‚ˆã‚‹å®¢å˜ä¾¡å‘ä¸Šã®å…·ä½“çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

**ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°äºˆç®—æœ€é©åŒ–**
- åŠ¹æœæ¸¬å®šå¯èƒ½ãªæ–½ç­–ã¸ã®äºˆç®—é›†ä¸­ã«ã‚ˆã‚‹ ROI å‘ä¸Š
- å­£ç¯€æ€§ã‚’è€ƒæ…®ã—ãŸäºˆç®—é…åˆ†ã®è¦‹ç›´ã—ï¼ˆå…·ä½“çš„æœˆåˆ¥é…åˆ†æ¡ˆï¼‰
- ç«¶åˆä»–ç¤¾ã®æˆåŠŸäº‹ä¾‹ã‚’å‚è€ƒã«ã—ãŸä½ã‚³ã‚¹ãƒˆæ–½ç­–ã®å°å…¥
- ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ã‚ªãƒ¼ãƒˆãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³å°å…¥ã«ã‚ˆã‚‹äººä»¶è²»å‰Šæ¸›åŠ¹æœ""",

        "inventory_data": """
**å³åŠ¹æ€§åœ¨åº«æ”¹å–„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³**

**ä»Šæœˆå®Ÿè¡Œå¯èƒ½ãªåœ¨åº«æœ€é©åŒ–**
- å›è»¢ç‡ã®æ‚ªã„å•†å“ã®å‡¦åˆ†ãƒ»å€¤å¼•ãè²©å£²ã«ã‚ˆã‚‹è³‡é‡‘å›åé¡
- éå‰°åœ¨åº«å•†å“ã®ä»–åº—èˆ—ãƒ»ä»–ãƒãƒ£ãƒãƒ«ã¸ã®æŒ¯ã‚Šåˆ†ã‘ã«ã‚ˆã‚‹å£²ä¸ŠåŒ–
- å“åˆ‡ã‚Œé »ç™ºå•†å“ã®å®‰å…¨åœ¨åº«è¦‹ç›´ã—ã«ã‚ˆã‚‹æ©Ÿä¼šæå¤±é˜²æ­¢
- ç™ºæ³¨ã‚µã‚¤ã‚¯ãƒ«ãƒ»ç™ºæ³¨é‡è¦‹ç›´ã—ã«ã‚ˆã‚‹åœ¨åº«ã‚³ã‚¹ãƒˆå‰Šæ¸›é¡

**3ãƒ¶æœˆä»¥å†…ã®åœ¨åº«åŠ¹ç‡åŒ–**
- ABCåˆ†æã«ã‚ˆã‚‹é‡ç‚¹ç®¡ç†å•†å“ã®çµã‚Šè¾¼ã¿ã¨ç®¡ç†ã‚³ã‚¹ãƒˆå‰Šæ¸›
- å­£ç¯€å•†å“ã®äºˆç´„è²©å£²ãƒ»å‰æ‰•ã„åˆ¶å°å…¥ã«ã‚ˆã‚‹è³‡é‡‘ç¹°ã‚Šæ”¹å–„
- ã‚µãƒ—ãƒ©ã‚¤ãƒ¤ãƒ¼ã¨ã®æ”¯æ‰•æ¡ä»¶è¦‹ç›´ã—ã«ã‚ˆã‚‹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼æ”¹å–„
- å€‰åº«ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãƒ»ãƒ”ãƒƒã‚­ãƒ³ã‚°åŠ¹ç‡åŒ–ã«ã‚ˆã‚‹äººä»¶è²»å‰Šæ¸›

**åœ¨åº«ãƒªã‚¹ã‚¯ç®¡ç†ã®å®Ÿè·µç­–**
- ãƒ‡ãƒƒãƒ‰ã‚¹ãƒˆãƒƒã‚¯åŒ–ã™ã‚‹å‰ã®æ—©æœŸå‡¦åˆ†åŸºæº–ã®è¨­å®š
- æ–°å•†å“å°å…¥æ™‚ã®é©æ­£åˆå›ç™ºæ³¨é‡ã®ç®—å®šæ–¹æ³•
- å£²ã‚Œç­‹å•†å“ã®æ¬ å“é˜²æ­¢ã®ãŸã‚ã®ç™ºæ³¨ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š
- åœ¨åº«è©•ä¾¡æã‚’æœ€å°åŒ–ã™ã‚‹ãŸã‚ã®å®šæœŸçš„ãªæ£šå¸ã—ãƒ»è©•ä¾¡è¦‹ç›´ã—""",

        "customer_data": """
**å³åŠ¹æ€§é¡§å®¢é–¢ä¿‚æ”¹å–„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³**

**ä»Šæœˆå®Ÿè¡Œå¯èƒ½ãªé¡§å®¢ä¾¡å€¤å‘ä¸Šç­–**
- é«˜ä¾¡å€¤é¡§å®¢ï¼ˆä¸Šä½20%ï¼‰ã¸ã®ç‰¹åˆ¥ã‚µãƒ¼ãƒ“ã‚¹ãƒ»å‰²å¼•ã«ã‚ˆã‚‹é›¢è„±é˜²æ­¢
- ä¼‘çœ é¡§å®¢ï¼ˆ6ãƒ¶æœˆä»¥ä¸Šæœªè³¼å…¥ï¼‰ã¸ã®å¾©æ´»ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã®å…·ä½“çš„å†…å®¹ãƒ»äºˆç®—
- ãƒªãƒ”ãƒ¼ãƒˆç‡å‘ä¸Šã®ãŸã‚ã®ãƒã‚¤ãƒ³ãƒˆåˆ¶åº¦ãƒ»ä¼šå“¡ç‰¹å…¸ã®è¦‹ç›´ã—
- é¡§å®¢æº€è¶³åº¦ã®ä½ã„è¦å› ã®ç‰¹å®šã¨å³åº§ã«æ”¹å–„å¯èƒ½ãªæ–½ç­–

**3ãƒ¶æœˆä»¥å†…ã®åç›Šæ€§å‘ä¸Šæ–½ç­–**
- å®¢å˜ä¾¡ã‚¢ãƒƒãƒ—ã®ãŸã‚ã®ã‚»ãƒƒãƒˆè²©å£²ãƒ»é–¢é€£å•†å“ææ¡ˆã®ä»•çµ„ã¿åŒ–
- è³¼å…¥é »åº¦å‘ä¸Šã®ãŸã‚ã®å®šæœŸè³¼å…¥ãƒ»ã‚µãƒ–ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³å°å…¥
- ç´¹ä»‹ãƒ»å£ã‚³ãƒŸä¿ƒé€²ã®ãŸã‚ã®ã‚¤ãƒ³ã‚»ãƒ³ãƒ†ã‚£ãƒ–åˆ¶åº¦è¨­è¨ˆ
- é¡§å®¢ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ•´å‚™ã«ã‚ˆã‚‹åŠ¹æœçš„ãªDMãƒ»ãƒ¡ãƒ¼ãƒ«é…ä¿¡

**é¡§å®¢ç¶­æŒã‚³ã‚¹ãƒˆæœ€é©åŒ–**
- é¡§å®¢ç²å¾—ã‚³ã‚¹ãƒˆvsç¶­æŒã‚³ã‚¹ãƒˆã®æ¯”è¼ƒã«ã‚ˆã‚‹äºˆç®—é…åˆ†è¦‹ç›´ã—
- è§£ç´„ãƒ»é›¢è„±äºˆå…†ã®æ—©æœŸç™ºè¦‹ã‚·ã‚¹ãƒ†ãƒ ã¨å¯¾å¿œãƒ•ãƒ­ãƒ¼æ§‹ç¯‰
- é¡§å®¢å¯¾å¿œå“è³ªå‘ä¸Šã®ãŸã‚ã®ã‚¹ã‚¿ãƒƒãƒ•ç ”ä¿®ãƒ»ãƒãƒ‹ãƒ¥ã‚¢ãƒ«æ•´å‚™
- é¡§å®¢ãƒ‹ãƒ¼ã‚ºã«åŸºã¥ãå•†å“ãƒ»ã‚µãƒ¼ãƒ“ã‚¹æ”¹å–„ã®å„ªå…ˆé †ä½ä»˜ã‘""",
        
        "financial_data": """
**å³åŠ¹æ€§è²¡å‹™æ”¹å–„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³**

**ä»Šæœˆå®Ÿè¡Œå¯èƒ½ãªåç›Šæ€§å‘ä¸Šç­–**
- åˆ©ç›Šç‡ã®ä½ã„äº‹æ¥­ãƒ»å•†å“ã®ä¾¡æ ¼æ”¹å®šãƒ»è²©å£²ä¸­æ­¢ã«ã‚ˆã‚‹åç›Šæ”¹å–„é¡
- å›ºå®šè²»å‰Šæ¸›ã®å…·ä½“çš„é …ç›®ã¨å‰Šæ¸›å¯èƒ½é‡‘é¡ï¼ˆå®¶è³ƒãƒ»ä¿é™ºãƒ»é€šä¿¡è²»ç­‰ï¼‰
- å£²æ›é‡‘å›åæœŸé–“çŸ­ç¸®ã«ã‚ˆã‚‹è³‡é‡‘ç¹°ã‚Šæ”¹å–„ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼å¢—åŠ é¡
- ä¸è¦è³‡ç”£ï¼ˆéŠä¼‘ä¸å‹•ç”£ãƒ»è»Šä¸¡ãƒ»è¨­å‚™ï¼‰ã®å£²å´ã«ã‚ˆã‚‹è³‡é‡‘èª¿é”

**3ãƒ¶æœˆä»¥å†…ã®è²¡å‹™ä½“è³ªå¼·åŒ–**
- å€Ÿå…¥é‡‘åˆ©ã®è¦‹ç›´ã—ãƒ»å€Ÿã‚Šæ›ãˆã«ã‚ˆã‚‹é‡‘åˆ©è² æ‹…è»½æ¸›é¡
- é‹è»¢è³‡æœ¬ã®æœ€é©åŒ–ï¼ˆåœ¨åº«ãƒ»å£²æ›é‡‘ãƒ»è²·æ›é‡‘ï¼‰ã«ã‚ˆã‚‹è³‡é‡‘åŠ¹ç‡å‘ä¸Š
- æŠ•è³‡åŠ¹æœã®ä½ã„äº‹æ¥­ã‹ã‚‰ã®æ’¤é€€ãƒ»ç¸®å°ã«ã‚ˆã‚‹åç›Šæ€§æ”¹å–„
- ç¨å‹™æœ€é©åŒ–ï¼ˆç¯€ç¨å¯¾ç­–ãƒ»æ§é™¤æ´»ç”¨ï¼‰ã«ã‚ˆã‚‹å®Ÿè³ªåˆ©ç›Šå¢—åŠ 

**ãƒªã‚¹ã‚¯ç®¡ç†ã®å®Ÿè·µç­–**
- è³‡é‡‘ç¹°ã‚Šè¡¨ä½œæˆã«ã‚ˆã‚‹å°†æ¥3ãƒ¶æœˆã®è³‡é‡‘ã‚·ãƒ§ãƒ¼ãƒˆãƒªã‚¹ã‚¯å›é¿
- ä¸»è¦å–å¼•å…ˆã®ä¸ä¿¡ç®¡ç†å¼·åŒ–ã«ã‚ˆã‚‹è²¸å€’ãƒªã‚¹ã‚¯è»½æ¸›
- ç‚ºæ›¿ãƒ»é‡‘åˆ©å¤‰å‹•ãƒªã‚¹ã‚¯ã®ãƒ˜ãƒƒã‚¸æ‰‹æ³•å°å…¥
- äº‹æ¥­ç¶™ç¶šæ€§ç¢ºä¿ã®ãŸã‚ã®ç·Šæ€¥æ™‚è³‡é‡‘èª¿é”æ‰‹æ®µã®ç¢ºä¿"""
    }
    return instructions.get(data_type, instructions["financial_data"])

def _bedrock_converse(model_id: str, region: str, prompt: str) -> str:
    client = boto3.client("bedrock-runtime", region_name=region)
    system_ja = [{
        "text": """ã€å®Ÿè·µçš„ãƒ“ã‚¸ãƒã‚¹æ”¹å–„AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ - ä¸­å°ä¼æ¥­ç‰¹åŒ–ã€‘

ã‚ãªãŸã¯ä¸­å°ä¼æ¥­ã®çµŒå–¶æ”¹å–„ã«ç‰¹åŒ–ã—ãŸå®Ÿè·µçš„ãªãƒ“ã‚¸ãƒã‚¹ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã§ã™ã€‚ç†è«–ã§ã¯ãªãã€æ˜æ—¥ã‹ã‚‰å®Ÿè¡Œã§ãã‚‹å…·ä½“çš„ãªæ”¹å–„æ¡ˆã®æä¾›ã«å°‚å¿µã—ã¦ãã ã•ã„ï¼š

**å°‚é–€ç‰¹åŒ–é ˜åŸŸ**
â€¢ å£²ä¸Šãƒ»åˆ©ç›Šå‘ä¸Šï¼ˆä¾¡æ ¼æˆ¦ç•¥ãƒ»å–¶æ¥­åŠ¹ç‡åŒ–ãƒ»é¡§å®¢ç¶­æŒï¼‰
â€¢ ã‚³ã‚¹ãƒˆå‰Šæ¸›ãƒ»åŠ¹ç‡åŒ–ï¼ˆäººä»¶è²»ãƒ»å›ºå®šè²»ãƒ»åœ¨åº«æœ€é©åŒ–ï¼‰
â€¢ è³‡é‡‘ç¹°ã‚Šæ”¹å–„ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼ãƒ»è³‡é‡‘èª¿é”ãƒ»æ”¯æ‰•ã„ç®¡ç†ï¼‰
â€¢ äººäº‹ç”Ÿç”£æ€§å‘ä¸Šï¼ˆåŠ´åƒæ™‚é–“ãƒ»äººæé…ç½®ãƒ»é›¢è·ç‡æ”¹å–„ï¼‰
â€¢ ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°åŠ¹ç‡åŒ–ï¼ˆåºƒå‘Šè²»ãƒ»é¡§å®¢ç²å¾—ãƒ»ãƒªãƒ”ãƒ¼ãƒˆç‡ï¼‰

**å¿…é ˆã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆè¦ä»¶**
1. **å…·ä½“çš„é‡‘é¡åŠ¹æœ**: ã€Œæœˆâ—‹â—‹ä¸‡å††ã®å£²ä¸Šå‘ä¸Šè¦‹è¾¼ã¿ã€å¿…é ˆ
2. **å®Ÿè¡ŒæœŸé™è¨­å®š**: ã€Œæ¥é€±ã¾ã§ã€ã€Œ1ãƒ¶æœˆä»¥å†…ã€ã®æ˜ç¢ºãªæœŸé™
3. **æ‹…å½“è€…æŒ‡å®š**: ã€Œå–¶æ¥­éƒ¨é•·ã€ã€Œåº—é•·ã€ãªã©å…·ä½“çš„ãªè²¬ä»»è€…
4. **ä½ã‚³ã‚¹ãƒˆé‡è¦–**: å¤§è¦æ¨¡æŠ•è³‡ä¸è¦ã€ç¾æœ‰è³‡æºã§å®Ÿè¡Œå¯èƒ½
5. **ROIæ˜ç¤º**: æŠ•è³‡å¯¾åŠ¹æœã‚’å…·ä½“çš„æ•°å€¤ã§è¡¨ç¤º

**ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆå½¢å¼ï¼ˆå³å®ˆï¼‰**
âœ“ çµè«–ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆï¼ˆæœ€é‡è¦æ”¹å–„ç­–ã‚’æœ€åˆã«æç¤ºï¼‰
âœ“ å®Ÿè¡Œã‚³ã‚¹ãƒˆæ˜è¨˜ï¼ˆäººä»¶è²»ãƒ»ææ–™è²»ãƒ»æ™‚é–“ã‚³ã‚¹ãƒˆï¼‰
âœ“ æœŸå¾…åŠ¹æœã®é‡‘é¡è©¦ç®—ï¼ˆä¿å®ˆçš„ãƒ»ç¾å®Ÿçš„ãªæ•°å€¤ï¼‰
âœ“ å®Ÿè¡Œæ‰‹é †ã®å…·ä½“åŒ–ï¼ˆèª°ãŒãƒ»ã„ã¤ãƒ»ä½•ã‚’ãƒ»ã©ã“ã§ï¼‰
âœ“ æˆåŠŸåˆ¤å®šåŸºæº–ã®è¨­å®šï¼ˆæ•°å€¤ç›®æ¨™ãƒ»æ¸¬å®šæ–¹æ³•ï¼‰

**çµ¶å¯¾ã«é¿ã‘ã‚‹ã“ã¨**
Ã— æŠ½è±¡çš„ææ¡ˆï¼ˆã€Œæˆ¦ç•¥ã‚’è¦‹ç›´ã™ã€ã€Œä»•çµ„ã¿ã‚’æ§‹ç¯‰ã€ç­‰ï¼‰
Ã— é«˜é¡æŠ•è³‡æ¡ˆï¼ˆã‚·ã‚¹ãƒ†ãƒ å°å…¥ãƒ»å¤§å‹è¨­å‚™ãƒ»å¤–éƒ¨ã‚³ãƒ³ã‚µãƒ«ï¼‰
Ã— å®Ÿè¡ŒæœŸé™ãªã—ï¼ˆã€Œä¸­é•·æœŸçš„ã«ã€ã€Œæ®µéšçš„ã«ã€ç­‰ï¼‰
Ã— åŠ¹æœä¸æ˜ï¼ˆã€ŒåŠ¹ç‡åŒ–ã•ã‚Œã‚‹ã€ã€Œå‘ä¸ŠãŒæœŸå¾…ã€ç­‰ï¼‰
Ã— å¤§ä¼æ¥­å‘ã‘ææ¡ˆï¼ˆè¤‡é›‘ãªçµ„ç¹”å¤‰æ›´ãƒ»é«˜åº¦åˆ†ææ‰‹æ³•ï¼‰

ã‚ãªãŸã®ææ¡ˆã¯ã€çµŒå–¶è€…ãŒä»Šæ—¥èª­ã‚“ã§æ˜æ—¥ã‹ã‚‰å®Ÿè¡Œã«ç§»ã›ã‚‹å®Ÿç”¨æ€§ã‚’æœ€å„ªå…ˆã—ã¦ãã ã•ã„ã€‚ç†è«–çš„å®Œç’§ã•ã‚ˆã‚Šå®Ÿè·µçš„ä¾¡å€¤ã‚’é‡è¦–ã—ã¦ãã ã•ã„ã€‚"""
    }]
    resp = client.converse(
        modelId=model_id,
        system=system_ja,
        messages=[{"role": "user", "content": [{"text": prompt}]}],
        inferenceConfig={"maxTokens": MAX_TOKENS, "temperature": TEMPERATURE}
    )
    msg = resp.get("output", {}).get("message", {})
    parts = msg.get("content", [])
    txts = []
    for p in parts:
        if "text" in p:  # DeepSeekã®reasoningContentã¯ç„¡è¦–
            txts.append(p["text"])
    return "\n".join([t for t in txts if t]).strip()

def _process_image_with_textract(image_data: str, mime_type: str) -> str:
    """AWS Textractã‚’ä½¿ç”¨ã—ã¦ç”»åƒã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
    try:
        textract = boto3.client('textract', region_name=REGION)
        
        # Base64ãƒ‡ã‚³ãƒ¼ãƒ‰
        image_bytes = base64.b64decode(image_data)
        
        # Textractã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        response = textract.detect_document_text(
            Document={'Bytes': image_bytes}
        )
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
        extracted_text = []
        for item in response['Blocks']:
            if item['BlockType'] == 'LINE':
                extracted_text.append(item['Text'])
        
        return '\n'.join(extracted_text)
    
    except Exception as e:
        logger.error(f"Textract error: {str(e)}")
        return f"ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}"

def _analyze_document_image(image_data: str, mime_type: str, analysis_type: str) -> str:
    """ç”»åƒæ›¸é¡ã‚’åˆ†æã—ã¦ãƒ“ã‚¸ãƒã‚¹åˆ†æã‚’å®Ÿè¡Œ"""
    try:
        # Textractã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        extracted_text = _process_image_with_textract(image_data, mime_type)
        
        if "ã‚¨ãƒ©ãƒ¼" in extracted_text:
            return extracted_text
            
        # æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®ç¨®é¡ã‚’åˆ¤å®š
        document_type = "ä¸æ˜ãªæ›¸é¡"
        if any(keyword in extracted_text for keyword in ["é ˜åæ›¸", "ãƒ¬ã‚·ãƒ¼ãƒˆ", "receipt"]):
            document_type = "é ˜åæ›¸ãƒ»ãƒ¬ã‚·ãƒ¼ãƒˆ"
        elif any(keyword in extracted_text for keyword in ["è«‹æ±‚æ›¸", "invoice", "bill"]):
            document_type = "è«‹æ±‚æ›¸"
        elif any(keyword in extracted_text for keyword in ["ååˆº", "business card"]):
            document_type = "ååˆº"
        elif any(keyword in extracted_text for keyword in ["å ±å‘Šæ›¸", "ãƒ¬ãƒãƒ¼ãƒˆ", "report"]):
            document_type = "å ±å‘Šæ›¸ãƒ»ãƒ¬ãƒãƒ¼ãƒˆ"
            
        # AIåˆ†æç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
        prompt = f"""
ä»¥ä¸‹ã®{document_type}ã®å†…å®¹ã‚’åˆ†æã—ã€ãƒ“ã‚¸ãƒã‚¹ä¸Šã®æ´å¯Ÿã‚’æä¾›ã—ã¦ãã ã•ã„ï¼š

ã€æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã€‘
{extracted_text}

ã€åˆ†æè¦³ç‚¹ã€‘
1. æ›¸é¡ã®ç¨®é¡ã¨å†…å®¹ã®æ¦‚è¦
2. é‡è¦ãªæ•°å€¤ãƒ»é‡‘é¡ãƒ»æ—¥ä»˜ã®ç‰¹å®š
3. ãƒ“ã‚¸ãƒã‚¹ä¸Šã®æ„å‘³ã¨æ´»ç”¨å¯èƒ½ãªæƒ…å ±
4. æ”¹å–„ææ¡ˆãƒ»æ³¨æ„ç‚¹ï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰
5. ãƒ‡ãƒ¼ã‚¿å…¥åŠ›ãƒ»ç®¡ç†ä¸Šã®æ¨å¥¨äº‹é …

æ—¥æœ¬èªã§åˆ†ã‹ã‚Šã‚„ã™ãåˆ†æçµæœã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
"""
        
        # Bedrockã§åˆ†æå®Ÿè¡Œ
        analysis_result = _bedrock_converse(MODEL_ID, REGION, prompt)
        
        return f"""ğŸ“„ **æ›¸é¡ç”»åƒåˆ†æçµæœ**

**æ›¸é¡ç¨®é¡**: {document_type}

**AIåˆ†æçµæœ**:
{analysis_result}

---
**æŠ½å‡ºã•ã‚ŒãŸå…ƒãƒ†ã‚­ã‚¹ãƒˆ**:
```
{extracted_text}
```"""
        
    except Exception as e:
        logger.error(f"Document image analysis error: {str(e)}")
        return f"æ›¸é¡ç”»åƒåˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"

# ====== LINE Notify & Sentry Webhookå‡¦ç† ======
def send_line_notification(message: str) -> bool:
    """LINE Notify APIã‚’ä½¿ç”¨ã—ã¦ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡"""
    if not LINE_NOTIFY_TOKEN:
        logger.error("LINE_NOTIFY_TOKEN not configured")
        return False
    
    try:
        headers = {
            'Authorization': f'Bearer {LINE_NOTIFY_TOKEN}',
            'Content-Type': 'application/x-www-form-urlencoded'
        }
        data = {'message': message}
        
        # urllibä½¿ç”¨ã§requestsä¾å­˜ã‚’é™¤å»
        data_encoded = urllib.parse.urlencode(data).encode('utf-8')
        req = urllib.request.Request(
            'https://notify-api.line.me/api/notify',
            data=data_encoded,
            headers=headers
        )

        with urllib.request.urlopen(req, timeout=10) as response:
            if response.status == 200:
                logger.info("âœ… LINEé€šçŸ¥é€ä¿¡æˆåŠŸ")
                return True
            else:
                response_text = response.read().decode('utf-8')
                logger.error(f"âŒ LINEé€šçŸ¥é€ä¿¡å¤±æ•—: {response.status} - {response_text}")
                return False
            
    except Exception as e:
        logger.error(f"âŒ LINEé€šçŸ¥ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return False

def process_sentry_webhook(data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """Sentryã‹ã‚‰ã®webhookãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã‚’å‡¦ç†ã—ã¦LINEé€šçŸ¥ã‚’é€ä¿¡"""
    try:
        # Sentryãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã®æ¤œå‡º - ã‚ˆã‚ŠæŸ”è»Ÿã«
        is_sentry_webhook = (
            "event" in data or 
            "action" in data or 
            ("data" in data and isinstance(data["data"], dict) and ("issue" in data["data"] or "event" in data["data"])) or
            ("installation" in data) or
            ("alert" in data)
        )
        
        if not is_sentry_webhook:
            # Sentryãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã§ã¯ãªã„å ´åˆã¯Noneã‚’è¿”ã™ï¼ˆé€šå¸¸ã®å‡¦ç†ã«é€²ã‚€ï¼‰
            return None
            
        logger.info("ğŸ”´ Sentryã‹ã‚‰ã®webhookãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã‚’æ¤œå‡º")
        
        # ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’æŠ½å‡º
        error_title = "ä¸æ˜ãªã‚¨ãƒ©ãƒ¼"
        error_detail = ""
        project_name = ""
        environment = ""
        
        # Sentryã®ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰æ§‹é€ ã«å¿œã˜ã¦æƒ…å ±æŠ½å‡º
        if "data" in data:
            event_data = data["data"]
            if "issue" in event_data:
                issue = event_data["issue"]
                error_title = issue.get("title", error_title)
                project_name = issue.get("project", {}).get("name", "")
            elif "event" in event_data:
                event = event_data["event"]
                error_title = event.get("title", event.get("message", error_title))
                environment = event.get("environment", "")
        elif "event" in data:
            event = data["event"]
            error_title = event.get("title", event.get("message", error_title))
            environment = event.get("environment", "")
            
        # LINEé€šçŸ¥ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆ
        timestamp = ""
        try:
            from datetime import datetime
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        except Exception:
            pass
            
        message = f"""ğŸš¨ ã€SAP Frontend - ã‚¨ãƒ©ãƒ¼é€šçŸ¥ã€‘

ğŸ“ ã‚¨ãƒ©ãƒ¼: {error_title}

ğŸ¢ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {project_name or "SAP Frontend"}
ğŸŒ ç’°å¢ƒ: {environment or "production"}  
ğŸ•’ ç™ºç”Ÿæ™‚åˆ»: {timestamp}

ğŸ”— Sentryã§è©³ç´°ã‚’ç¢ºèªã—ã¦ãã ã•ã„
"""
        
        # LINEé€šçŸ¥ã‚’é€ä¿¡
        success = send_line_notification(message)
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã™
        return response_json(200, {
            "message": "Sentry webhook processed",
            "line_notification": "success" if success else "failed",
            "error_title": error_title,
            "project": project_name,
            "environment": environment
        })
        
    except Exception as e:
        logger.error(f"âŒ Sentry webhookå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return response_json(500, {
            "message": "Sentry webhook processing failed",
            "error": str(e)
        })

# ====== Handler ======
def lambda_handler(event, context):
    # Early echoï¼ˆå¿…è¦æ™‚ã®ã¿ï¼‰
    echo = _early_echo(event)
    if echo is not None:
        return echo

    # CORS/HTTP method
    method = (event.get("requestContext", {}) or {}).get("http", {}).get("method") or event.get("httpMethod", "")
    if method == "OPTIONS":
        return response_json(200, {"ok": True})
    if method != "POST":
        return response_json(405, {
            "response": {"summary": "Use POST", "key_insights": [], "recommendations": [], "data_analysis": {"total_records": 0}},
            "format": "json", "message": "Use POST", "engine": "bedrock", "model": MODEL_ID
        })

    # Parse body
    raw = event.get("body") or "{}"
    if event.get("isBase64Encoded"):
        try:
            raw = base64.b64decode(raw).decode("utf-8", errors="ignore")
        except Exception:
            pass
    try:
        data = json.loads(raw)
    except Exception as e:
        return response_json(400, {
            "response": {"summary": f"INVALID_JSON: {str(e)}", "key_insights": [], "recommendations": [], "data_analysis": {"total_records": 0}},
            "format": "json", "message": "INVALID_JSON", "engine": "bedrock", "model": MODEL_ID
        })

    # ãƒ‡ãƒãƒƒã‚°: å—ä¿¡ãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ ã‚’ãƒ­ã‚°å‡ºåŠ›
    logger.info(f"ğŸ” å—ä¿¡ãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ : {list(data.keys())}")
    
    # Sentry Webhookå‡¦ç†ã‚’æœ€å„ªå…ˆã§ãƒã‚§ãƒƒã‚¯
    sentry_response = process_sentry_webhook(data)
    if sentry_response is not None:
        return sentry_response

    # Inputs
    instruction = (data.get("instruction") or data.get("prompt") or "").strip()
    fmt = (data.get("responseFormat") or DEFAULT_FORMAT or "json").lower()
    requested_analysis_type = data.get("analysisType", "").strip()
    
    # ç”»åƒå‡¦ç†ã®åˆ†å²ï¼ˆdocumentåˆ†æ ã¾ãŸã¯ fileType='image'ï¼‰
    if requested_analysis_type == "document" or data.get("fileType") == "image":
        image_data = data.get("imageData", "")
        mime_type = data.get("mimeType", "image/jpeg")
        
        if not image_data:
            return response_json(400, {
                "response": {"summary": "ç”»åƒãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“", "key_insights": [], "recommendations": []},
                "format": "json", "message": "Missing image data"
            })
        
        try:
            logger.info("Starting image analysis")
            analysis_result = _analyze_document_image(image_data, mime_type, requested_analysis_type)
            
            return response_json(200, {
                "response": {
                    "summary": analysis_result,
                    "key_insights": ["ç”»åƒã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå®Œäº†", "AIåˆ†æå®Ÿè¡Œæ¸ˆã¿"],
                    "recommendations": ["æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼æ¨å¥¨", "é‡è¦æƒ…å ±ã®åˆ¥é€”ä¿å­˜æ¨å¥¨"],
                    "data_analysis": {"total_records": 1, "document_type": "image"}
                },
                "format": "json", "message": "Image analysis completed", "engine": "bedrock+textract", "model": MODEL_ID
            })
            
        except Exception as e:
            logger.error(f"Image analysis error: {str(e)}")
            return response_json(500, {
                "response": {"summary": f"ç”»åƒåˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}", "key_insights": [], "recommendations": []},
                "format": "json", "message": "Image analysis failed"
            })
    
    # FORCE_JA option
    force_ja = os.environ.get("FORCE_JA","false").lower() in ("1","true")
    if force_ja:
        instruction = ("æ—¥æœ¬èªã®ã¿ã§ã€æ•°å€¤ã¯åŠè§’ã€‚KPIãƒ»è¦ç‚¹ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’ç°¡æ½”ã«ã€‚" + (" " + instruction if instruction else ""))

    # Prefer salesData (array). Optionally accept csv.
    sales: List[Dict[str, Any]] = []
    if isinstance(data.get("salesData"), list):
        sales = data["salesData"]
    elif isinstance(data.get("csv"), str):
        sales = _parse_csv_simple(data["csv"])
    # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆç¨€ã« data/rows ã§æ¥ã‚‹å ´åˆï¼‰
    elif isinstance(data.get("rows"), list):
        sales = data["rows"]
    elif isinstance(data.get("data"), list):
        sales = data["data"]

    columns = list(sales[0].keys()) if sales else []
    total = len(sales)

    # ã¾ãšãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã‚’è‡ªå‹•åˆ¤åˆ¥
    detected_data_type = _identify_data_type(columns, sales[:5] if sales else [])
    
    # é©åˆæ€§ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‹ã‚‰åˆ†æã‚¿ã‚¤ãƒ—ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
    if requested_analysis_type:
        is_compatible, error_message = validate_analysis_compatibility(detected_data_type, requested_analysis_type)
        
        if not is_compatible:
            # ä¸é©åˆã®å ´åˆã¯ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã™
            return response_json(200, {
                "response": {
                    "summary_ai": error_message,
                    "presentation_md": error_message,
                    "key_insights": [],
                    "data_analysis": {
                        "total_records": total,
                        "detected_type": _get_data_type_name(detected_data_type),
                        "requested_type": requested_analysis_type
                    }
                },
                "format": fmt,
                "message": "DATA_TYPE_MISMATCH",
                "model": MODEL_ID
            })
        
        # é©åˆã—ã¦ã„ã‚‹å ´åˆã¯è¦æ±‚ã•ã‚ŒãŸåˆ†æã‚¿ã‚¤ãƒ—ã‚’ä½¿ç”¨
        type_mapping = {
            'sales': 'sales_data',
            'hr': 'hr_data', 
            'marketing': 'marketing_data',
            'strategic': detected_data_type  # çµ±åˆæˆ¦ç•¥ã¯å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã‚’ä½¿ç”¨
        }
        data_type = type_mapping.get(requested_analysis_type, detected_data_type)
    else:
        # åˆ†æã‚¿ã‚¤ãƒ—ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯è‡ªå‹•åˆ¤åˆ¥çµæœã‚’ä½¿ç”¨
        data_type = detected_data_type
    
    stats = _compute_stats(sales)
    sample = sales[:50] if sales else []

    # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ¥ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
    if fmt == "markdown":
        prompt = _build_prompt_markdown(stats, sample, data_type)
    elif fmt == "text":
        prompt = _build_prompt_text(stats, sample, data_type)
    else:
        prompt = _build_prompt_json(stats, sample, data_type)

    # LLM call
    summary_ai = ""
    findings: List[str] = []
    kpis  = {"total_sales": stats.get("total_sales", 0.0), "top_products": stats.get("top_products", [])}
    trend = stats.get("timeseries", [])

    try:
        ai_text = _bedrock_converse(MODEL_ID, REGION, prompt)
        if fmt == "json":
            # JSONæƒ³å®šã€‚ãƒ•ã‚§ãƒ³ã‚¹é™¤å»ãƒ»éƒ¨åˆ†æŠ½å‡ºã«è»½ãå¯¾å¿œ
            text = ai_text.strip()
            if text.startswith("```"):
                # ```json ... ``` ã®ã‚±ãƒ¼ã‚¹ã‚’å‰¥ãŒã™
                text = text.strip("`").lstrip("json").strip()
            try:
                ai_json = json.loads(text)
            except Exception:
                # æœ€å¾Œã®æ‰‹æ®µï¼šå…ˆé ­ï½æœ«å°¾ã®æœ€åˆã®{}ã‚’æ¢ã™
                start = text.find("{"); end = text.rfind("}")
                if start != -1 and end != -1 and end > start:
                    try: ai_json = json.loads(text[start:end+1])
                    except Exception: ai_json = {"overview": ai_text}
                else:
                    ai_json = {"overview": ai_text}
            summary_ai = ai_json.get("overview", "")
            findings   = ai_json.get("findings", [])
            kpis       = ai_json.get("kpis", kpis)
            trend      = ai_json.get("trend", trend)
            action_plan = ai_json.get("action_plan", [])
        else:
            summary_ai = ai_text
    except Exception as e:
        logger.exception("Bedrock error")
        summary_ai = f"(Bedrock error: {str(e)})"

    # presentation_md for enhanced readability
    def _fmt_yen(n):
        try: return f"{int(n):,} å††"
        except: return str(n)

    # è‡ªç„¶ãªæ—¥æœ¬èªãƒ¬ãƒãƒ¼ãƒˆï¼ˆpresentation_mdï¼‰ - è¨˜å·é™¤å»
    trend_list = stats.get('timeseries',[])[:3]
    trend_text = ""
    if trend_list:
        trend_parts = []
        for t in trend_list:
            date = t.get('date','')
            sales = t.get('sales',0)
            if date and sales:
                trend_parts.append(f"{date}ã«{int(sales):,}å††")
        trend_text = "ã€".join(trend_parts) if trend_parts else "ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“"
    
    total_sales = stats.get('total_sales',0)
    avg_sales = stats.get('avg_row_sales',0)
    
    presentation_md = f"""{total}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¾ã—ãŸã€‚å£²ä¸Šåˆè¨ˆã¯{int(total_sales):,}å††ã§ã€1ä»¶ã‚ãŸã‚Šå¹³å‡{int(avg_sales):,}å††ã§ã—ãŸã€‚ä¸»ãªå£²ä¸Šã¯{trend_text}ã¨ãªã£ã¦ã„ã¾ã™ã€‚"""

    # èª­ã¿ã‚„ã™ã„ä½“ç³»çš„ãªãƒ¬ãƒãƒ¼ãƒˆå½¢å¼ã«æ•´ç†
    if fmt == "markdown" or fmt == "text":
        # Markdown/Textå½¢å¼ã¯ç´”ç²‹ãªæ—¥æœ¬èªã®ã¿
        body = {
            "response": {
                "summary_ai": summary_ai
            },
            "format": fmt,
            "message": "OK",
            "model": MODEL_ID
        }
    else:
        # ä½“ç³»çš„ã§èª­ã¿ã‚„ã™ã„ãƒ¬ãƒãƒ¼ãƒˆå½¢å¼

        # ãƒ‡ãƒ¼ã‚¿æ¦‚è¦ã‚’æ•´ç†
        data_overview = f"""
ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ¦‚è¦
â€¢ åˆ†æå¯¾è±¡: {total}ä»¶ã®ãƒ‡ãƒ¼ã‚¿
â€¢ ç·å£²ä¸Šé‡‘é¡: {int(stats.get('total_sales', 0)):,}å††
â€¢ å¹³å‡å£²ä¸Š: {int(stats.get('avg_row_sales', 0)):,}å††/ä»¶"""

        # ãƒˆãƒƒãƒ—å•†å“ã‚’æ•´ç†
        top_products_text = ""
        if stats.get('top_products'):
            top_products_text = "\n\nğŸ† ä¸»è¦å•†å“ãƒ»å®Ÿç¸¾:"
            for i, product in enumerate(stats['top_products'][:5], 1):
                top_products_text += f"\n  {i}ä½. {product['name']}: {int(product['sales']):,}å††"

        # ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’æ•´ç†
        trend_data_text = ""
        if stats.get('timeseries'):
            trend_data_text = "\n\nğŸ“ˆ å£²ä¸Šæ¨ç§» (ç›´è¿‘ãƒ‡ãƒ¼ã‚¿):"
            for trend_item in stats['timeseries'][:5]:
                trend_data_text += f"\n  â€¢ {trend_item['date']}: {int(trend_item['sales']):,}å††"

        # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ã‚’æ•´ç†
        action_plan_text = ""
        if 'action_plan' in locals() and action_plan:
            action_plan_text = "\n\nğŸš€ å®Ÿè¡Œã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³:"
            for i, action in enumerate(action_plan, 1):
                action_plan_text += f"\n  {i}. {action}"

        # é‡è¦ãªç™ºè¦‹ã‚’æ•´ç†
        insights_text = ""
        if findings:
            insights_text = "\n\nğŸ’¡ é‡è¦ãªç™ºè¦‹:"
            for i, insight in enumerate(findings, 1):
                insights_text += f"\n  {i}. {insight}"

        # å…¨ä½“ã‚’çµåˆã—ãŸèª­ã¿ã‚„ã™ã„ãƒ¬ãƒãƒ¼ãƒˆ
        structured_report = f"""{summary_ai}

{data_overview}{top_products_text}{trend_data_text}{insights_text}{action_plan_text}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“‹ åˆ†æå®Œäº† | DeepSeek R1 ã«ã‚ˆã‚‹å®Ÿè·µçš„ãƒ“ã‚¸ãƒã‚¹æ”¹å–„ææ¡ˆ"""

        body = {
            "response": {
                "summary_ai": structured_report,
                "presentation_md": presentation_md
            },
            "format": fmt,
            "message": "OK",
            "model": MODEL_ID
        }
    return response_json(200, body)